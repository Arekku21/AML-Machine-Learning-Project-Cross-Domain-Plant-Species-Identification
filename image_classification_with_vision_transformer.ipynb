{"cells":[{"cell_type":"markdown","metadata":{"id":"642ffCm0TImk"},"source":["# Image classification with Vision Transformer\n","\n","**Based on:** [Image classification with Visiion Transformer](https://keras.io/examples/vision/image_classification_with_vision_transformer/)<br>"]},{"cell_type":"markdown","metadata":{"id":"3jUKydCnTImu"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4214,"status":"ok","timestamp":1670414480750,"user":{"displayName":"lol lol","userId":"00635959247292470070"},"user_tz":-480},"id":"waItmhhpTxHD","outputId":"b812f240-4689-44a9-cdc9-eb42d4fba461"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.8/dist-packages (0.18.0)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (2.7.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n"]}],"source":["pip install -U tensorflow-addons #required for the AdamW optimiser"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nths7CTaTImw"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import tensorflow_addons as tfa\n","import shutil\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4152,"status":"ok","timestamp":1670414484891,"user":{"displayName":"lol lol","userId":"00635959247292470070"},"user_tz":-480},"id":"3cErzCcwTPKp","outputId":"50131617-f59c-47de-bdd8-8a819361d32f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at Mydrive; to attempt to forcibly remount, call drive.mount(\"Mydrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('Mydrive')"]},{"cell_type":"markdown","metadata":{"id":"A68qPbd9TImy"},"source":["## Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_3gl1h1WTImy"},"outputs":[],"source":["learning_rate = 0.001\n","weight_decay = 0.0001\n","batch_size = 64\n","num_epochs = 50\n","image_size = 224  # We'll resize input images to this size\n","patch_size = 16  # Size of the patches to be extract from the input images\n","num_patches = (image_size // patch_size) ** 2\n","projection_dim = 64\n","num_heads = 4\n","transformer_units = [\n","    projection_dim * 2,\n","    projection_dim,\n","]  # Size of the transformer layers\n","transformer_layers = 3\n","mlp_head_units = [256, 128]  # Size of the dense layers of the final classifier\n","\n","train_path=\"/content/Mydrive/MyDrive/Colab Notebooks/AML_project/train\"\n","test_path=\"/content/Mydrive/MyDrive/Colab Notebooks/AML_project/test\"\n","#IMG_SIZE = 224 # MobilenetV2\n","RESOLUTION = 224\n","PATCH_SIZE = 16\n","\n","num_classes = 100\n","input_shape = (image_size, image_size, 3)"]},{"cell_type":"markdown","source":["##Getting Dataset"],"metadata":{"id":"FbmzuyRQ6a_U"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"D6QPDNBwUqa_"},"outputs":[],"source":["#declare plant species dictionary\n","plant_species_dict = {}\n","with open('/content/Mydrive/MyDrive/Colab Notebooks/AML_project/list/species_list.txt') as txt_file:\n","    lines =  [x.strip() for x in txt_file.readlines()]\n","    plant_class = [x.split('; ')[0] for x in lines]\n","    plant_species = [x.split('; ')[1] for x in lines]\n","\n","for i in range(len(plant_class)):\n","    plant_species_dict[plant_class[i]] = plant_species[i]\n","\n","#plant_species_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":556,"status":"ok","timestamp":1670414485435,"user":{"displayName":"lol lol","userId":"00635959247292470070"},"user_tz":-480},"id":"U1qwDa_lTY_-","outputId":"0fce882d-240b-491a-90ad-eefa9d939d16"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 3018 files belonging to 100 classes.\n","Found 1933 files belonging to 100 classes.\n"]}],"source":["x_train = tf.keras.utils.image_dataset_from_directory(train_path + \"/herbarium\",\n","                                                            shuffle=True,\n","                                                            batch_size=batch_size,\n","                                                            image_size=(image_size, image_size),\n","                                                            )\n","                                                      \n","x_test = tf.keras.utils.image_dataset_from_directory(test_path,\n","                                                                 shuffle=True,\n","                                                                 batch_size=batch_size,\n","                                                                 image_size=(image_size, image_size)\n","                                                                 )\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tfDxr5paTIm1"},"source":["## Use data augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MTdBI7OcTIm1"},"outputs":[],"source":["data_augmentation = keras.Sequential(\n","    [\n","        layers.Normalization(),\n","        layers.RandomFlip(\"horizontal\"),\n","        layers.RandomRotation(factor=0.02),\n","    ],\n","    name=\"data_augmentation\",\n",")\n","# Compute the mean and the variance of the training data for normalization.\n"]},{"cell_type":"markdown","metadata":{"id":"cjaCfOYCTIm2"},"source":["## Implement multilayer perceptron (MLP)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7tEcHjDbTIm2"},"outputs":[],"source":["\n","def mlp(x, hidden_units, dropout_rate):\n","    for units in hidden_units:\n","        x = layers.Dense(units, activation='softmax')(x)\n","        x = layers.Dropout(dropout_rate)(x)\n","    return x\n"]},{"cell_type":"markdown","metadata":{"id":"sbiUd47HTIm3"},"source":["## Implement patch creation as a layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ANUKIwxhTIm3"},"outputs":[],"source":["\n","class Patches(layers.Layer):\n","    def __init__(self, patch_size):\n","        super(Patches, self).__init__()\n","        self.patch_size = patch_size\n","\n","    def call(self, images):\n","        batch_size = tf.shape(images)[0]\n","        patches = tf.image.extract_patches(\n","            images=images,\n","            sizes=[1, self.patch_size, self.patch_size, 1],\n","            strides=[1, self.patch_size, self.patch_size, 1],\n","            rates=[1, 1, 1, 1],\n","            padding=\"VALID\",\n","        )\n","        patch_dims = patches.shape[-1]\n","        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n","        return patches\n"]},{"cell_type":"markdown","metadata":{"id":"OSBLR_cmTIm4"},"source":["## Implement the patch encoding layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gqp8-MrfTIm4"},"outputs":[],"source":["\n","class PatchEncoder(layers.Layer):\n","    def __init__(self, num_patches, projection_dim):\n","        super(PatchEncoder, self).__init__()\n","        self.num_patches = num_patches\n","        self.projection = layers.Dense(units=projection_dim)\n","        self.position_embedding = layers.Embedding(\n","            input_dim=num_patches, output_dim=projection_dim\n","        )\n","\n","    def call(self, patch):\n","        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n","        encoded = self.projection(patch) + self.position_embedding(positions)\n","        return encoded\n"]},{"cell_type":"markdown","metadata":{"id":"8qa07vgcTIm5"},"source":["## Build the ViT model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jSIolQeITIm5"},"outputs":[],"source":["def create_vit_classifier():\n","    inputs = layers.Input(shape=input_shape)\n","    # Augment data.\n","    augmented = data_augmentation(inputs)\n","    # Create patches.\n","    patches = Patches(patch_size)(augmented)\n","    # Encode patches.\n","    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n","\n","    # Create multiple layers of the Transformer block.\n","    for _ in range(transformer_layers):\n","        # Layer normalization 1.\n","        x1 = layers.LayerNormalization()(encoded_patches)\n","        # Create a multi-head attention layer.\n","        attention_output = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n","        )(x1, x1)\n","        # Skip connection 1.\n","        x2 = layers.Add()([attention_output, encoded_patches])\n","        # Layer normalization 2.\n","        x3 = layers.LayerNormalization()(x2)\n","        # MLP.\n","        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n","        # Skip connection 2.\n","        encoded_patches = layers.Add()([x3, x2])\n","\n","    # Create a [batch_size, projection_dim] tensor.\n","    representation = layers.LayerNormalization()(encoded_patches)\n","    representation = layers.Flatten()(representation)\n","    #representation = layers.Dropout(0.2)(representation)\n","    # Add MLP.\n","    #features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.2)\n","    features = layers.Dense(256)(representation)\n","    features = layers.Dense(128)(features)\n","    features = layers.Dropout(0.2)(features)\n","    # Classify outputs.\n","    logits = layers.Dense(num_classes, kernel_regularizer=tf.keras.regularizers.l2(0.1))(features)\n","    # Create the Keras model\n","    model = keras.Model(inputs=inputs, outputs=logits)\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"dJlubBszTIm5"},"source":["## Compile, train, and evaluate the mode"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"643boZgFTIm5","outputId":"25e966d9-ae02-452b-e3dd-e9db1cda0231","executionInfo":{"status":"ok","timestamp":1670416176120,"user_tz":-480,"elapsed":1689323,"user":{"displayName":"lol lol","userId":"00635959247292470070"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","48/48 [==============================] - 35s 604ms/step - loss: 29.4526 - accuracy: 0.0126 - top-5-accuracy: 0.0593 - val_loss: 16.0080 - val_accuracy: 0.0129 - val_top-5-accuracy: 0.0662\n","Epoch 2/50\n","48/48 [==============================] - 25s 473ms/step - loss: 14.6030 - accuracy: 0.0265 - top-5-accuracy: 0.0934 - val_loss: 14.1038 - val_accuracy: 0.0062 - val_top-5-accuracy: 0.0621\n","Epoch 3/50\n","48/48 [==============================] - 25s 467ms/step - loss: 12.7704 - accuracy: 0.0361 - top-5-accuracy: 0.1153 - val_loss: 12.3142 - val_accuracy: 0.0129 - val_top-5-accuracy: 0.0807\n","Epoch 4/50\n","48/48 [==============================] - 25s 478ms/step - loss: 11.3348 - accuracy: 0.0391 - top-5-accuracy: 0.1382 - val_loss: 10.7959 - val_accuracy: 0.0150 - val_top-5-accuracy: 0.0988\n","Epoch 5/50\n","48/48 [==============================] - 25s 472ms/step - loss: 9.7696 - accuracy: 0.0570 - top-5-accuracy: 0.1657 - val_loss: 9.8313 - val_accuracy: 0.0140 - val_top-5-accuracy: 0.0885\n","Epoch 6/50\n","48/48 [==============================] - 25s 476ms/step - loss: 8.6103 - accuracy: 0.0749 - top-5-accuracy: 0.2005 - val_loss: 8.3459 - val_accuracy: 0.0667 - val_top-5-accuracy: 0.1433\n","Epoch 7/50\n","48/48 [==============================] - 25s 469ms/step - loss: 7.6063 - accuracy: 0.0915 - top-5-accuracy: 0.2353 - val_loss: 7.7370 - val_accuracy: 0.0523 - val_top-5-accuracy: 0.1417\n","Epoch 8/50\n","48/48 [==============================] - 25s 471ms/step - loss: 6.8340 - accuracy: 0.1107 - top-5-accuracy: 0.2644 - val_loss: 6.9491 - val_accuracy: 0.0507 - val_top-5-accuracy: 0.1899\n","Epoch 9/50\n","48/48 [==============================] - 25s 474ms/step - loss: 6.1985 - accuracy: 0.1322 - top-5-accuracy: 0.3065 - val_loss: 6.4571 - val_accuracy: 0.0714 - val_top-5-accuracy: 0.1956\n","Epoch 10/50\n","48/48 [==============================] - 25s 474ms/step - loss: 5.6091 - accuracy: 0.1581 - top-5-accuracy: 0.3436 - val_loss: 6.0558 - val_accuracy: 0.0911 - val_top-5-accuracy: 0.2344\n","Epoch 11/50\n","48/48 [==============================] - 25s 474ms/step - loss: 5.2077 - accuracy: 0.1799 - top-5-accuracy: 0.3810 - val_loss: 5.8177 - val_accuracy: 0.0802 - val_top-5-accuracy: 0.2183\n","Epoch 12/50\n","48/48 [==============================] - 25s 477ms/step - loss: 4.8637 - accuracy: 0.1862 - top-5-accuracy: 0.4089 - val_loss: 5.5178 - val_accuracy: 0.1004 - val_top-5-accuracy: 0.2649\n","Epoch 13/50\n","48/48 [==============================] - 25s 468ms/step - loss: 4.5546 - accuracy: 0.2104 - top-5-accuracy: 0.4437 - val_loss: 5.4812 - val_accuracy: 0.0885 - val_top-5-accuracy: 0.2276\n","Epoch 14/50\n","48/48 [==============================] - 25s 471ms/step - loss: 4.3085 - accuracy: 0.2263 - top-5-accuracy: 0.4589 - val_loss: 5.2263 - val_accuracy: 0.0921 - val_top-5-accuracy: 0.2607\n","Epoch 15/50\n","48/48 [==============================] - 25s 477ms/step - loss: 4.1010 - accuracy: 0.2382 - top-5-accuracy: 0.4914 - val_loss: 5.1422 - val_accuracy: 0.1045 - val_top-5-accuracy: 0.2514\n","Epoch 16/50\n","48/48 [==============================] - 25s 475ms/step - loss: 3.9618 - accuracy: 0.2452 - top-5-accuracy: 0.4927 - val_loss: 4.9461 - val_accuracy: 0.1081 - val_top-5-accuracy: 0.2892\n","Epoch 17/50\n","48/48 [==============================] - 25s 471ms/step - loss: 3.7727 - accuracy: 0.2614 - top-5-accuracy: 0.5149 - val_loss: 4.9708 - val_accuracy: 0.0967 - val_top-5-accuracy: 0.2545\n","Epoch 18/50\n","48/48 [==============================] - 25s 473ms/step - loss: 3.6840 - accuracy: 0.2691 - top-5-accuracy: 0.5351 - val_loss: 4.8987 - val_accuracy: 0.1242 - val_top-5-accuracy: 0.2887\n","Epoch 19/50\n","48/48 [==============================] - 25s 468ms/step - loss: 3.5251 - accuracy: 0.3055 - top-5-accuracy: 0.5510 - val_loss: 4.9251 - val_accuracy: 0.1081 - val_top-5-accuracy: 0.2814\n","Epoch 20/50\n","48/48 [==============================] - 24s 466ms/step - loss: 3.4416 - accuracy: 0.3002 - top-5-accuracy: 0.5616 - val_loss: 4.8449 - val_accuracy: 0.1205 - val_top-5-accuracy: 0.2856\n","Epoch 21/50\n","48/48 [==============================] - 24s 465ms/step - loss: 3.3636 - accuracy: 0.3135 - top-5-accuracy: 0.5779 - val_loss: 4.8029 - val_accuracy: 0.1086 - val_top-5-accuracy: 0.2928\n","Epoch 22/50\n","48/48 [==============================] - 24s 466ms/step - loss: 3.2746 - accuracy: 0.3323 - top-5-accuracy: 0.5974 - val_loss: 4.8498 - val_accuracy: 0.0983 - val_top-5-accuracy: 0.2783\n","Epoch 23/50\n","48/48 [==============================] - 25s 470ms/step - loss: 3.1633 - accuracy: 0.3413 - top-5-accuracy: 0.6166 - val_loss: 4.9118 - val_accuracy: 0.0978 - val_top-5-accuracy: 0.2514\n","Epoch 24/50\n","48/48 [==============================] - 24s 465ms/step - loss: 3.0981 - accuracy: 0.3509 - top-5-accuracy: 0.6259 - val_loss: 4.8794 - val_accuracy: 0.1029 - val_top-5-accuracy: 0.2980\n","Epoch 25/50\n","48/48 [==============================] - 24s 463ms/step - loss: 3.0643 - accuracy: 0.3615 - top-5-accuracy: 0.6359 - val_loss: 4.8314 - val_accuracy: 0.1148 - val_top-5-accuracy: 0.2938\n","Epoch 26/50\n","48/48 [==============================] - 24s 467ms/step - loss: 3.0611 - accuracy: 0.3506 - top-5-accuracy: 0.6262 - val_loss: 4.9003 - val_accuracy: 0.1123 - val_top-5-accuracy: 0.2840\n","Epoch 27/50\n","48/48 [==============================] - 25s 469ms/step - loss: 2.9636 - accuracy: 0.3761 - top-5-accuracy: 0.6491 - val_loss: 4.7971 - val_accuracy: 0.1138 - val_top-5-accuracy: 0.2938\n","Epoch 28/50\n","48/48 [==============================] - 25s 468ms/step - loss: 2.8832 - accuracy: 0.3910 - top-5-accuracy: 0.6650 - val_loss: 4.8385 - val_accuracy: 0.1009 - val_top-5-accuracy: 0.2763\n","Epoch 29/50\n","48/48 [==============================] - 25s 473ms/step - loss: 2.8242 - accuracy: 0.3970 - top-5-accuracy: 0.6803 - val_loss: 4.8750 - val_accuracy: 0.1309 - val_top-5-accuracy: 0.3130\n","Epoch 30/50\n","48/48 [==============================] - 25s 469ms/step - loss: 2.7697 - accuracy: 0.4175 - top-5-accuracy: 0.6899 - val_loss: 5.0837 - val_accuracy: 0.0879 - val_top-5-accuracy: 0.2706\n","Epoch 31/50\n","48/48 [==============================] - 24s 465ms/step - loss: 2.8200 - accuracy: 0.4056 - top-5-accuracy: 0.6789 - val_loss: 4.8056 - val_accuracy: 0.1148 - val_top-5-accuracy: 0.2933\n","Epoch 32/50\n","48/48 [==============================] - 25s 473ms/step - loss: 2.6636 - accuracy: 0.4314 - top-5-accuracy: 0.7124 - val_loss: 4.8346 - val_accuracy: 0.1097 - val_top-5-accuracy: 0.2990\n","Epoch 33/50\n","48/48 [==============================] - 24s 465ms/step - loss: 2.6265 - accuracy: 0.4317 - top-5-accuracy: 0.7134 - val_loss: 4.9181 - val_accuracy: 0.1169 - val_top-5-accuracy: 0.2990\n","Epoch 34/50\n","48/48 [==============================] - 24s 464ms/step - loss: 2.6017 - accuracy: 0.4440 - top-5-accuracy: 0.7213 - val_loss: 4.9775 - val_accuracy: 0.1040 - val_top-5-accuracy: 0.2799\n","Epoch 35/50\n","48/48 [==============================] - 24s 464ms/step - loss: 2.5166 - accuracy: 0.4695 - top-5-accuracy: 0.7286 - val_loss: 4.9915 - val_accuracy: 0.1029 - val_top-5-accuracy: 0.2907\n","Epoch 36/50\n","48/48 [==============================] - 24s 462ms/step - loss: 2.5193 - accuracy: 0.4655 - top-5-accuracy: 0.7439 - val_loss: 5.0348 - val_accuracy: 0.1159 - val_top-5-accuracy: 0.3037\n","Epoch 37/50\n","48/48 [==============================] - 24s 462ms/step - loss: 2.4399 - accuracy: 0.4715 - top-5-accuracy: 0.7492 - val_loss: 5.0663 - val_accuracy: 0.1242 - val_top-5-accuracy: 0.3104\n","Epoch 38/50\n","48/48 [==============================] - 24s 463ms/step - loss: 2.3853 - accuracy: 0.5000 - top-5-accuracy: 0.7594 - val_loss: 5.1502 - val_accuracy: 0.0952 - val_top-5-accuracy: 0.2809\n","Epoch 39/50\n","48/48 [==============================] - 25s 484ms/step - loss: 2.4124 - accuracy: 0.4838 - top-5-accuracy: 0.7565 - val_loss: 5.0593 - val_accuracy: 0.1081 - val_top-5-accuracy: 0.2938\n","Epoch 40/50\n","48/48 [==============================] - 24s 463ms/step - loss: 2.3279 - accuracy: 0.4973 - top-5-accuracy: 0.7740 - val_loss: 5.0528 - val_accuracy: 0.1283 - val_top-5-accuracy: 0.3114\n","Epoch 41/50\n","48/48 [==============================] - 25s 466ms/step - loss: 2.2958 - accuracy: 0.5103 - top-5-accuracy: 0.7787 - val_loss: 5.1304 - val_accuracy: 0.1061 - val_top-5-accuracy: 0.2850\n","Epoch 42/50\n","48/48 [==============================] - 24s 465ms/step - loss: 2.2617 - accuracy: 0.5192 - top-5-accuracy: 0.7866 - val_loss: 5.1689 - val_accuracy: 0.1221 - val_top-5-accuracy: 0.3104\n","Epoch 43/50\n","48/48 [==============================] - 24s 463ms/step - loss: 2.1891 - accuracy: 0.5351 - top-5-accuracy: 0.7959 - val_loss: 5.1193 - val_accuracy: 0.1185 - val_top-5-accuracy: 0.3052\n","Epoch 44/50\n","48/48 [==============================] - 26s 498ms/step - loss: 2.2158 - accuracy: 0.5275 - top-5-accuracy: 0.7952 - val_loss: 5.1016 - val_accuracy: 0.1195 - val_top-5-accuracy: 0.2845\n","Epoch 45/50\n","48/48 [==============================] - 24s 464ms/step - loss: 2.1470 - accuracy: 0.5451 - top-5-accuracy: 0.8025 - val_loss: 5.1633 - val_accuracy: 0.1128 - val_top-5-accuracy: 0.3130\n","Epoch 46/50\n","48/48 [==============================] - 24s 466ms/step - loss: 2.1406 - accuracy: 0.5424 - top-5-accuracy: 0.8005 - val_loss: 5.2156 - val_accuracy: 0.1174 - val_top-5-accuracy: 0.2923\n","Epoch 47/50\n","48/48 [==============================] - 24s 461ms/step - loss: 2.0737 - accuracy: 0.5679 - top-5-accuracy: 0.8194 - val_loss: 5.4792 - val_accuracy: 0.1148 - val_top-5-accuracy: 0.2949\n","Epoch 48/50\n","48/48 [==============================] - 24s 465ms/step - loss: 2.0359 - accuracy: 0.5742 - top-5-accuracy: 0.8237 - val_loss: 5.2381 - val_accuracy: 0.1169 - val_top-5-accuracy: 0.3099\n","Epoch 49/50\n","48/48 [==============================] - 24s 462ms/step - loss: 2.0509 - accuracy: 0.5726 - top-5-accuracy: 0.8221 - val_loss: 5.2237 - val_accuracy: 0.1205 - val_top-5-accuracy: 0.3259\n","Epoch 50/50\n","48/48 [==============================] - 25s 468ms/step - loss: 2.0547 - accuracy: 0.5630 - top-5-accuracy: 0.8201 - val_loss: 5.3246 - val_accuracy: 0.1361 - val_top-5-accuracy: 0.3063\n","31/31 [==============================] - 8s 197ms/step - loss: 5.3246 - accuracy: 0.1361 - top-5-accuracy: 0.3063\n","Test accuracy: 13.61%\n","Test top 5 accuracy: 30.63%\n"]}],"source":["\n","def run_experiment(model):\n","    optimizer = tfa.optimizers.AdamW(\n","        learning_rate=learning_rate, weight_decay=weight_decay\n","    )\n","    model.compile(\n","        optimizer=optimizer,\n","        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","        metrics=[\n","            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n","            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n","        ],\n","    )\n","\n","    checkpoint_filepath = \"/tmp/checkpoint\"\n","    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n","        checkpoint_filepath,\n","        monitor=\"val_accuracy\",\n","        save_best_only=True,\n","        save_weights_only=True,\n","    )\n","\n","    history = model.fit(\n","        x_train,\n","        epochs=num_epochs,\n","        validation_data = x_test,\n","        callbacks=[checkpoint_callback],\n","    )\n","\n","    model.load_weights(checkpoint_filepath)\n","    _, accuracy, top_5_accuracy = model.evaluate(x_test)\n","    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n","    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n","\n","    return history\n","\n","\n","vit_classifier = create_vit_classifier()\n","history = run_experiment(vit_classifier)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5KCoSLJif3fc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670416197564,"user_tz":-480,"elapsed":21457,"user":{"displayName":"lol lol","userId":"00635959247292470070"}},"outputId":"45263901-81e7-46ce-de73-f2adcaac36e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["48/48 [==============================] - 14s 241ms/step - loss: 1.4034 - accuracy: 0.7445 - top-5-accuracy: 0.9281\n","31/31 [==============================] - 8s 199ms/step - loss: 5.3246 - accuracy: 0.1361 - top-5-accuracy: 0.3063\n"]}],"source":["train_acc = vit_classifier.evaluate(x_train)\n","test_acc = vit_classifier.evaluate(x_test )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_J_AbAdDRpfB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670416197565,"user_tz":-480,"elapsed":23,"user":{"displayName":"lol lol","userId":"00635959247292470070"}},"outputId":"2ac87cde-e6be-4a87-fa82-0999503fc677"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train accuracy: 74.4532823562622\n","Test accuracy: 13.605794310569763\n"]}],"source":["print(f'Train accuracy: {train_acc[1]*100}')\n","print(f'Test accuracy: {test_acc[1]*100}')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/vision/ipynb/image_classification_with_vision_transformer.ipynb","timestamp":1669925215818}]},"environment":{"name":"tf2-gpu.2-4.m61","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"nbformat":4,"nbformat_minor":0}